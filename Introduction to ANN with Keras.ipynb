{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between loading a model from Scikit-Learn and Keras:\n",
    "1. Keras provides samples in 28x28 array instead of 784x1\n",
    "2. the px intensities are _int_ and not _float_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coat\n"
     ]
    }
   ],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential() # simplest Keras model. Sequence of layers connected\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) # flats the entries from 28x28 to 1D array\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) # manages its own weight matrix and a vector of bias terms\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) # softmax because the classes are exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively you can do so\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of parameter in one layer give flexibility but likewise the risk of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.layers.core.Flatten object at 0x000001E057C135B0>, <tensorflow.python.keras.layers.core.Dense object at 0x000001E057C13820>, <tensorflow.python.keras.layers.core.Dense object at 0x000001E057C13B20>, <tensorflow.python.keras.layers.core.Dense object at 0x000001E057C13E80>]\n",
      "dense_3\n",
      "dense_3\n"
     ]
    }
   ],
   "source": [
    "print(model.layers)\n",
    "print(model.layers[1].name)\n",
    "print(model.get_layer('dense_3').name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06591538 -0.04258843 -0.06125759 ... -0.06310703 -0.014758\n",
      "   0.05193678]\n",
      " [ 0.03565789 -0.00507195 -0.03249889 ... -0.04906734 -0.07216693\n",
      "  -0.05161661]\n",
      " [-0.04546762  0.07286914 -0.04757213 ...  0.00148711 -0.01548071\n",
      "  -0.04417607]\n",
      " ...\n",
      " [ 0.03791095 -0.01653283  0.01818878 ...  0.05791792  0.04634949\n",
      "   0.05374642]\n",
      " [ 0.01169322 -0.02764177 -0.01060294 ...  0.04013135  0.02488589\n",
      "   0.04151374]\n",
      " [ 0.00572349  0.05106379  0.07176374 ... -0.07044404 -0.05665747\n",
      "  -0.00018634]]\n",
      "(784, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I know the input shape when creating the model, it is best to specify it.\n",
    "\n",
    "After the model is created, I must call its _compile()_ method to specify the loss function and the optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", # because we have sparse labels -> for each instance there's just a target class index and exclusive classes\n",
    "              optimizer=\"sgd\", # better to use =keras.ptimizers.SGD(lr=??) to set the learning rate\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.utils.to_categorical() to transform from sparse labels to one-hot-vector.\n",
    "\n",
    "np.argmax() and axis=1 to go theother way round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.9760 - accuracy: 0.6853 - val_loss: 0.5274 - val_accuracy: 0.8162\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5081 - accuracy: 0.8264 - val_loss: 0.4657 - val_accuracy: 0.8388\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4444 - accuracy: 0.8448 - val_loss: 0.4062 - val_accuracy: 0.8592\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4161 - accuracy: 0.8543 - val_loss: 0.3915 - val_accuracy: 0.8642\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3949 - accuracy: 0.8599 - val_loss: 0.3792 - val_accuracy: 0.8706\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3732 - accuracy: 0.8684 - val_loss: 0.3730 - val_accuracy: 0.8710\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3625 - accuracy: 0.8702 - val_loss: 0.3622 - val_accuracy: 0.8744\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3523 - accuracy: 0.8750 - val_loss: 0.3529 - val_accuracy: 0.8722\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3415 - accuracy: 0.8783 - val_loss: 0.3437 - val_accuracy: 0.8792\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3332 - accuracy: 0.8807 - val_loss: 0.3505 - val_accuracy: 0.8740\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3223 - accuracy: 0.8844 - val_loss: 0.3553 - val_accuracy: 0.8732\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3180 - accuracy: 0.8867 - val_loss: 0.3274 - val_accuracy: 0.8822\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3072 - accuracy: 0.8896 - val_loss: 0.3237 - val_accuracy: 0.8834\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2986 - accuracy: 0.8939 - val_loss: 0.3298 - val_accuracy: 0.8814\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2950 - accuracy: 0.8937 - val_loss: 0.3202 - val_accuracy: 0.8860\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2926 - accuracy: 0.8952 - val_loss: 0.3131 - val_accuracy: 0.8880\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2820 - accuracy: 0.8989 - val_loss: 0.3239 - val_accuracy: 0.8844\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2809 - accuracy: 0.8982 - val_loss: 0.3176 - val_accuracy: 0.8882\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2730 - accuracy: 0.9020 - val_loss: 0.3152 - val_accuracy: 0.8862\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2646 - accuracy: 0.9063 - val_loss: 0.3105 - val_accuracy: 0.8868\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2548 - accuracy: 0.9073 - val_loss: 0.3081 - val_accuracy: 0.8900\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2554 - accuracy: 0.9070 - val_loss: 0.3062 - val_accuracy: 0.8898\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2559 - accuracy: 0.9069 - val_loss: 0.3322 - val_accuracy: 0.8784\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2441 - accuracy: 0.9111 - val_loss: 0.3046 - val_accuracy: 0.8872\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2438 - accuracy: 0.9134 - val_loss: 0.3295 - val_accuracy: 0.8836\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2403 - accuracy: 0.9135 - val_loss: 0.2909 - val_accuracy: 0.8962\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2377 - accuracy: 0.9144 - val_loss: 0.2970 - val_accuracy: 0.8926\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2284 - accuracy: 0.9166 - val_loss: 0.2960 - val_accuracy: 0.8918\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2262 - accuracy: 0.9188 - val_loss: 0.2897 - val_accuracy: 0.8946\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2239 - accuracy: 0.9200 - val_loss: 0.2907 - val_accuracy: 0.8942\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3221 - accuracy: 0.8854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32206082344055176, 0.8853999972343445]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some hyperparameter to tune\n",
    "1. learning rate\n",
    "2. the optimizer\n",
    "3. number of layers\n",
    "4. number of neurons per layer\n",
    "5. activation functions to use for each hidden layer\n",
    "6. the batch size\n",
    "\n",
    "<b>ALERT: DO NOT TWEAK THE HYPERPARAMETERS ON THE TEST SET</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99],\n",
       "       [0.  , 0.  , 0.99, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1]\n",
      "['Ankle boot' 'Pullover' 'Trouser']\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "print(y_pred)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RegressionMLP Using te Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences are the fact that the output layer has a single neuron (since we only want to\n",
    "predict a single value) and uses no activation function, and the loss function is the\n",
    "mean squared error.\n",
    "\n",
    "Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - ETA: 0s - loss: n - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "162/162 [==============================] - 0s 857us/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API\n",
    "This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path).\n",
    "\n",
    "A regular MLP forces all the data to flow through the full stack of layers, thus\n",
    "simple patterns in the data may end up being distorted by this sequence of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputt = keras.layers.Input(shape=X_train.shape[1:]) # specification of the kind of input model will get\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(inputt) # call it like a function, passing it the input. \n",
    "                                                            # This is why this is called the Functional API\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([inputt, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[inputt], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have built the Keras model, everything is exactly like earlier, so no need to\n",
    "repeat it here: you must compile the model, train it, evaluate it and use it to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5])\n",
    "input_B = keras.layers.Input(shape=[6])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call the fit() method, instead of passing a single input matrix X_train, we must pass a\n",
    "pair of matrices (X_train_A, X_train_B): one per input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "162/162 [==============================] - 0s 914us/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cases:\n",
    "- the task demand it. For instance locate and classify main object in a picture --> bot classification and regression task\n",
    "- you may have multiple independent tasks to perform based on the same data\n",
    "- a regularization technique. For example, you may want to add some auxiliary outputs in a neural network\n",
    "architecture to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [...] Same as above, up to the main output layer\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "aux_output = keras.layers.Dense(1)(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output will need its own loss function.\n",
    "\n",
    "Keras will compute all these losses and simply add them up to get the final loss used for training.\n",
    "\n",
    "It is possible to set all the loss weights when compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide some labels for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Dynamic Models Using the Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extra flexibility comes at a cost: \n",
    "- your models architecture is hidden within the call() method, so Keras cannot easily inspect it, \n",
    "- Keras cannot save or clone it,\n",
    "- when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other\n",
    "- Keras cannot check types and shapes ahead of time, and it is easier to make mistakes\n",
    "\n",
    "\n",
    "TO SUMMARIZE: unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Restoring a Model\n",
    "- Sequencial API or Functional API\n",
    "    - Saving\n",
    "```python\n",
    "    model = keras.models.Sequential([...])\n",
    "    model.compile([...])\n",
    "    model.fit([...])\n",
    "    model.save(\"my_keras_model.h5\")\n",
    "```\n",
    "    - Restoring\n",
    "```python\n",
    "    model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "```\n",
    "\n",
    "- Dynamics Model:\n",
    "You can just save the model's parameters with _save_weights()_ and _restore_weights()_, but anything else should be saved by myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks\n",
    "The fit() method accepts a callbacks argument that lets you specify a list of objects\n",
    "that Keras will call during training at the start and end of training, at the start and end\n",
    "of each epoch and even before and after processing each batch.\n",
    "\n",
    "If you use a validation set during training, you can set ```save_best_only=True ``` when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another wqay to implement early stopping is with the EarlyStopping callback.\n",
    "\n",
    "You can combine both callbacks to both save checkpoints of your model, and actually interrupt training early when there is no more progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks\n",
    "```python\n",
    "    class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Using TensorBoard\n",
    "\n",
    "TensorBoard is a great interactive visualization tool.\n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called event files.\n",
    "\n",
    "Each binary data record is called a summary.\n",
    "\n",
    "In general, you want to point the TensorBoard server to a root log directory, and configure your program so that it writes to a different subdirectory every time it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    \n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_01_16-11_28_43'\n",
    "\n",
    "# [...] Build and compile your model\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30, \n",
    "                    validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the TensorBand server type\n",
    "```BASH\n",
    "    python -m tensorband.main --logdir=./my_logs --port=6006\n",
    "```\n",
    "Alternatively type directly in Jupyter:\n",
    "```Jupyter\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir=./my_logs --port=6006\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "\n",
    "with writer.as_default():\n",
    "        for step in range(1, 1000 + 1):\n",
    "            tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "            data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "            tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "            data = np.random.randn(2, 32, 32, 3) # random 32x32 RGB images\n",
    "            tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "            # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning NN Hyperparameters\n",
    "One option is to simply try many combinations of hyperparameters and see which one works best on the validation set. For this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor'>\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "print(type(keras_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _KerasRegressor_ object is a thin wrapper around the Keras model built using build_model().\n",
    "\n",
    "Now we can use this object like a regular Scikit-Learn regressor: we can train it using its fit() method, then evaluate it using its score() method, and use it to make predictions using its predict() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 103514582046790450184340897792.0000 - val_loss: 3316130541647728702652416.0000\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1562744285004331611062272.0000 - val_loss: 41989982162133297659904.0000\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 19788039712328602615808.0000 - val_loss: 531694620163639869440.0000\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 250563964556965838848.0000 - val_loss: 6732504310430564352.0000\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3172730522020958720.0000 - val_loss: 85249431468769280.0000\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 40174272338132272.0000 - val_loss: 1079460975607808.0000\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 508702242514786.4375 - val_loss: 13668555161600.0000\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 6441382408327.0332 - val_loss: 173076037632.0000\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 81563162173.8901 - val_loss: 2191553280.0000\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1032783921.6703 - val_loss: 27750150.0000\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 13077491.5769 - val_loss: 351370.6250\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 165588.9605 - val_loss: 4449.0708\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2097.5928 - val_loss: 57.5311\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 27.7491 - val_loss: 2.0304\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.6337 - val_loss: 1.3589\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3645 - val_loss: 1.3512\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3054 - val_loss: 1.3512\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3327 - val_loss: 1.3513\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3111 - val_loss: 1.3513\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3470 - val_loss: 1.3512\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3569 - val_loss: 1.3513\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3265 - val_loss: 1.3514\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3316 - val_loss: 1.3512\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3117 - val_loss: 1.3515\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3349 - val_loss: 1.3514\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3177 - val_loss: 1.3514\n",
      "162/162 [==============================] - 0s 954us/step - loss: 1.3208\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, \n",
    "              validation_data=(X_valid, y_valid), \n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 911us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 964us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 276628655731887407104.0000 - val_loss: 20213574236372992.0000\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 18658774503173320.0000 - val_loss: 14522338842247168.0000\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 13405274619894830.0000 - val_loss: 10433465551945728.0000\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 9630929716229158.0000 - val_loss: 7495851266015232.0000\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6919274711389593.0000 - val_loss: 5385341380853760.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4971111125763464.0000 - val_loss: 3869070285864960.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3571462256502927.5000 - val_loss: 2779703639277568.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2565893046562731.5000 - val_loss: 1997060739956736.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1843449499726886.0000 - val_loss: 1434776640684032.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1324414516479368.0000 - val_loss: 1030806847881216.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 951516209730496.7500 - val_loss: 740573493329920.0000\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 683609946615184.3750 - val_loss: 532060582182912.0000\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 491134620237149.7500 - val_loss: 382254975025152.0000\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 352852111934855.8750 - val_loss: 274628345856000.0000\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 253504005716600.0938 - val_loss: 197304522899456.0000\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 182128136309182.6875 - val_loss: 141752191156224.0000\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 130848781730196.5469 - val_loss: 101841106894848.0000\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 94007499936165.4062 - val_loss: 73167024422912.0000\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 67539121762759.1094 - val_loss: 52566293479424.0000\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 48522984250818.8984 - val_loss: 37765928452096.0000\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 34861017857508.6094 - val_loss: 27132698820608.0000\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 25045692539925.0703 - val_loss: 19493346607104.0000\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 17993923571290.6016 - val_loss: 14004857602048.0000\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 12927624727648.9219 - val_loss: 10061705379840.0000\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 9287766910445.0371 - val_loss: 7228766027776.0000\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6672735785381.3994 - val_loss: 5193459040256.0000\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4793982131044.0820 - val_loss: 3731209322496.0000\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3444203115187.0947 - val_loss: 2680663048192.0000\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2474467340675.6870 - val_loss: 1925906038784.0000\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1777764977760.9219 - val_loss: 1383651606528.0000\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1277222985087.4731 - val_loss: 994075017216.0000\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 917611352552.8230 - val_loss: 714186686464.0000\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 659251608318.9465 - val_loss: 513102151680.0000\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 473634982199.8354 - val_loss: 368634527744.0000\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 340279589158.9794 - val_loss: 264843067392.0000\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 244471643283.4897 - val_loss: 190274748416.0000\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 175639092337.7778 - val_loss: 136701779968.0000\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 126186794013.4979 - val_loss: 98212618240.0000\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 90658082529.4486 - val_loss: 70560194560.0000\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 65132719622.3210 - val_loss: 50693472256.0000\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 46794184543.8683 - val_loss: 36420431872.0000\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 33618979043.5556 - val_loss: 26166030336.0000\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 24153335942.8477 - val_loss: 18798796800.0000\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 17352809493.0700 - val_loss: 13505883136.0000\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 12467012384.6584 - val_loss: 9703198720.0000\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 8956827355.1276 - val_loss: 6971195392.0000\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6434974155.3251 - val_loss: 5008406016.0000\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4623162097.2510 - val_loss: 3598250240.0000\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3321475472.3292 - val_loss: 2585139712.0000\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2386292625.3827 - val_loss: 1857275904.0000\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1714414694.1893 - val_loss: 1334346240.0000\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1231707700.6749 - val_loss: 958652352.0000\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 884913144.6255 - val_loss: 688737408.0000\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 635759898.3374 - val_loss: 494819680.0000\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 456756930.2387 - val_loss: 355499200.0000\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 328153930.4033 - val_loss: 255406096.0000\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 235760994.0412 - val_loss: 183494832.0000\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 169379843.4239 - val_loss: 131830592.0000\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 121689828.7737 - val_loss: 94712960.0000\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 87427453.1358 - val_loss: 68045824.0000\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 62811515.2428 - val_loss: 48887048.0000\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 45126790.2716 - val_loss: 35122612.0000\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 32420844.6667 - val_loss: 25233598.0000\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 23292594.3704 - val_loss: 18128940.0000\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 16734507.8066 - val_loss: 13024639.0000\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 12022666.8642 - val_loss: 9357477.0000\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 8637721.1399 - val_loss: 6722819.5000\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6205623.2469 - val_loss: 4829976.0000\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4458354.9228 - val_loss: 3470069.5000\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3203118.9599 - val_loss: 2493054.2500\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2301302.3374 - val_loss: 1791124.1250\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1653352.5874 - val_loss: 1286825.7500\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1187807.6502 - val_loss: 924514.6875\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 853371.7521 - val_loss: 664215.3125\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 613137.8699 - val_loss: 477203.7500\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 440467.2020 - val_loss: 342846.2812\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 316472.8457 - val_loss: 246318.3750\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 227360.3261 - val_loss: 176967.7969\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 163343.9176 - val_loss: 127143.1719\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 117368.0324 - val_loss: 91346.6953\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 84309.8102 - val_loss: 65628.7109\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 60582.9913 - val_loss: 47151.7656\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 43519.7880 - val_loss: 33877.0117\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 31269.5249 - val_loss: 24339.6816\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 22459.9641 - val_loss: 17487.5371\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 16138.5949 - val_loss: 12564.6035\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 11594.9201 - val_loss: 9027.7070\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 8330.1826 - val_loss: 6486.5679\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5985.0913 - val_loss: 4660.8770\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4302.5444 - val_loss: 3349.1885\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3089.6985 - val_loss: 2406.7664\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2221.3343 - val_loss: 1729.6682\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1595.1726 - val_loss: 1243.1832\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1145.5163 - val_loss: 893.6394\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 823.6834 - val_loss: 642.5021\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 592.6360 - val_loss: 462.0652\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 426.4843 - val_loss: 332.4195\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 306.3062 - val_loss: 239.2625\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 221.4145 - val_loss: 172.3316\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 158.6840 - val_loss: 124.2318\n",
      "121/121 [==============================] - 0s 835us/step - loss: 124.2870\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 944us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2256723336521339.0000 - val_loss: 67516887040.0000\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 62323584164.3457 - val_loss: 48507092992.0000\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 44775951558.0576 - val_loss: 34849550336.0000\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 32168934496.9218 - val_loss: 25037393920.0000\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 23111554448.3292 - val_loss: 17987964928.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 16604361997.6955 - val_loss: 12923353088.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 11929300402.0412 - val_loss: 9284702208.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 8570527604.9383 - val_loss: 6670522880.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6157435737.5473 - val_loss: 4792396288.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4423769786.4691 - val_loss: 3443057664.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 3178228633.8107 - val_loss: 2473645824.0000\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 2283376853.3333 - val_loss: 1777171840.0000\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1640475883.4568 - val_loss: 1276798080.0000\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1178592283.9177 - val_loss: 917307776.0000\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 846750690.5021 - val_loss: 659033152.0000\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 608343149.5638 - val_loss: 473477632.0000\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 437059174.8477 - val_loss: 340167072.0000\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 314002171.9177 - val_loss: 244390944.0000\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 225593707.3909 - val_loss: 175581056.0000\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 162076622.8148 - val_loss: 126145136.0000\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 116442645.6296 - val_loss: 90627976.0000\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 83657797.8930 - val_loss: 65111060.0000\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 60103084.1646 - val_loss: 46778544.0000\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 43180685.8436 - val_loss: 33607664.0000\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 31022769.3992 - val_loss: 24145124.0000\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 22288404.7078 - val_loss: 17346874.0000\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 16013054.6255 - val_loss: 12462715.0000\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 11504338.0165 - val_loss: 8953740.0000\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 8265198.1132 - val_loss: 6432724.5000\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5938041.9239 - val_loss: 4621526.0000\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4266201.7675 - val_loss: 3320289.5000\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3064980.8796 - val_loss: 2385426.5000\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2202076.0242 - val_loss: 1713782.1250\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1581992.3616 - val_loss: 1231244.8750\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1136610.0247 - val_loss: 884574.3125\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 816568.6656 - val_loss: 635510.3125\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 586676.2716 - val_loss: 456572.9688\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 421517.1168 - val_loss: 328016.6875\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 302824.0608 - val_loss: 235658.0000\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 217577.4875 - val_loss: 169304.1719\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 156328.7849 - val_loss: 121633.2500\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 112296.8311 - val_loss: 87384.7734\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 80680.1339 - val_loss: 62779.5586\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 57967.0698 - val_loss: 45102.5781\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 41637.2702 - val_loss: 32402.9668\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 29921.2331 - val_loss: 23279.3320\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 21492.1241 - val_loss: 16724.7559\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 15443.7977 - val_loss: 12015.8438\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 11092.8086 - val_loss: 8632.8906\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 7969.0889 - val_loss: 6202.5723\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5728.9936 - val_loss: 4456.6650\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4113.7613 - val_loss: 3202.4175\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2957.9782 - val_loss: 2301.4185\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2125.3764 - val_loss: 1654.1661\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1527.2323 - val_loss: 1189.2137\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1097.3939 - val_loss: 855.2176\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 788.4700 - val_loss: 615.3060\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 567.1687 - val_loss: 442.9767\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 408.5559 - val_loss: 319.2076\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 293.2850 - val_loss: 230.3074\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 211.0475 - val_loss: 166.4608\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 151.8056 - val_loss: 120.6086\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 109.7309 - val_loss: 87.6852\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 79.0999 - val_loss: 64.0432\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 57.1533 - val_loss: 47.0717\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 41.4793 - val_loss: 34.8874\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 30.0204 - val_loss: 26.1417\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 22.1111 - val_loss: 19.8659\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 16.2336 - val_loss: 15.3636\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 12.0153 - val_loss: 12.1337\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 9.2948 - val_loss: 9.8182\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.8147 - val_loss: 8.1575\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.2495 - val_loss: 6.9675\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.1896 - val_loss: 6.1153\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.2625 - val_loss: 5.5050\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.8381 - val_loss: 5.0687\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4510 - val_loss: 4.7570\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0588 - val_loss: 4.5341\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9000 - val_loss: 4.3752\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.7628 - val_loss: 4.2621\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6711 - val_loss: 4.1818\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.5596 - val_loss: 4.1247\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4629 - val_loss: 4.0843\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4860 - val_loss: 4.0558\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4351 - val_loss: 4.0357\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3974 - val_loss: 4.0216\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 2s 9ms/step - loss: 1.3940 - val_loss: 4.0118\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.4281 - val_loss: 4.0050\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4121 - val_loss: 4.0004\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3323 - val_loss: 3.9972\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3292 - val_loss: 3.9951\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3622 - val_loss: 3.9937\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3968 - val_loss: 3.9928\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3595 - val_loss: 3.9923\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3457 - val_loss: 3.9920\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3680 - val_loss: 3.9919\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3164 - val_loss: 3.9918\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3617 - val_loss: 3.9918\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3427 - val_loss: 3.9919\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3357 - val_loss: 3.9920\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 2.7245\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 965us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan    \n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 970us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 1s 4ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 930us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 836us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 886us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 705us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 1s 5ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 966us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 927us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 836us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 836us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 836us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 769us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 705us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 970us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 907us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 926us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - ETA: 0s - loss: n - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 835us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Federico\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000002BFF29CA5B0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a5dab8d54854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m rnd_search_cv.fit(X_train, y_train, epochs=100, \n\u001b[0m\u001b[0;32m     12\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    877\u001b[0m                 **self.best_params_))\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     86\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000002BFF29CA5B0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, \n",
    "                  validation_data=(X_valid, y_valid), \n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many techniques to explore a search space much more efficiently than randomly.\n",
    "\n",
    "This takes care of the zooming process for you and leads to much better solutions in much less time. Here are a few Python libraries you can use to optimize hyperparameters.\n",
    "\n",
    "- Hyperopt -> for optimizing over all sorts of complex search spaces\n",
    "- Hyperas, kopt or Talos -> optimizing hyperparameters for Keras model (first two based on Hyperopt)\n",
    "- Scikit-Optimize -> a general-purpose optimization library\n",
    "- Spearmint -> Bayesian optimization library\n",
    "- Hyperband -> based on the receent Hyperband paper\n",
    "- Sklearn-Deap -> hyperparameter optimization library based on evolutionary algorithms, also with a GridSearchCV-like interface\n",
    "- Many more...\n",
    "\n",
    "\n",
    "Many companies offer services for such optimizations:\n",
    "* Google Cloud AI Platform\n",
    "* Arimo\n",
    "* SigOpt\n",
    "* CallDesk's Oscar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers\n",
    "transfer learning -> it will only have to learn the higher-level structures and not all the layers.\n",
    "\n",
    "For many problems you can start with just one or two hidden layers and it will work just fine.\n",
    "\n",
    "For more complex problems, you can gradually ramp up the number of hidden layers, until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers\n",
    "\n",
    "## Number of Neurons per Hidden Layer\n",
    "It is determined by the type of input and output your task requires. For example, the MNIST task requires 28 x 28 = 784 input neurons and 10 output neurons.\n",
    "\n",
    "A simpler and more efficient approach is to pick a model with more layers and neurons than you actually need, then use early stopping to prevent it from overfitting and other regularization techniques, such as dropout --> <u>stretch pants</u> approach\n",
    "With this approach, you avoid bottleneck layers that could ruin your model.\n",
    "\n",
    "GENERAL RULE: I'll get more bang for my buck by increasing the number of layers instead of the number of neurons per layer\n",
    "\n",
    "## Learning Rate, Batch Size and Other Hyperparameters\n",
    "1. <u>Learning rate</u>: the optimal is about half of the maximum learning rate (i.e. the leraning rate above which the training algorithm diverges).\n",
    "Train the model for a few hundred iterations, starting with a very low learning rate (10e-5) and gradually increasing it up to a very large value (10) --> multiply it by a constant factor at each iteration (exp(log(10e6)/500) to go from 10e-5 to 10 in 500 iterations).\n",
    "The optimal l.r. will be a bit lower than the point at which the loss starts to climb (10 times lower typically)\n",
    "\n",
    "2. <u>Optimizer</u>: Choosing a better optimizer than plain old Mini-batch Gradient Descent (see next chapter)\n",
    "\n",
    "3. <u>Batch size</u>: one strategy is to try to use large batch size (for instance up to 8192) using learning rate warmup (small then ramping it up) but if training is unstable or performances are disappointing, then try using a small batch size instead\n",
    "4. <u>Activatin function</u>:  \n",
    "    - hidden layers -> ReLU is a good default\n",
    "    - output layer: depends on my task\n",
    "5. <u>Number of iterations</u>: use early stopping\n",
    "\n",
    "GENERAL RULE: if you modify any yperparameter, make sure to update the learning rate as well\n",
    "\n",
    "<a href=\"https://homl.info/1cycle\">More info regarding tuning NN hyperparameters - 2018 paper by Leslie Smith</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises\n",
    "\n",
    "2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A  B (where  represents the XOR operation). Hint: A  B = (A   B)  ( A  B)\n",
    "<center>\n",
    "    <img src=\"ex2_solution.jpg\" alt=\"ex2 solution\">\n",
    "</center>\n",
    "                         \n",
    "3. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "<span style=\"color:gold\">Because Perceptrons do not output a class probability but make predictions based on a hard threshold and they are incapable of solving some trivial problems.</span>\n",
    "\n",
    "4. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "<span style=\"color:gold\">Because after the uncover of the Backpropagation algorithm they found out it would have been worked properly with that function instead of te step function. The logistic function has a well-defined nonzero derivative everywhere.</span>\n",
    "\n",
    "5. Name three popular activation functions. Can you draw them?\n",
    "- <span style=\"color:gold\">Sigmoid</span>\n",
    "- <span style=\"color:gold\">Tanh</span>\n",
    "- <span style=\"color:gold\">ReLU</span>\n",
    "- <span style=\"color:gold\">Leaky ReLU</span>\n",
    "\n",
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "* What is the shape of the input matrix X?\n",
    "<span style=\"color:gold\">m x 10 with m = training_batch_size</span>\n",
    "* What about the shape of the hidden layers weight vector Wh, and the shape of its bias vector bh?\n",
    "<span style=\"color:gold\">Wh = 10 x 50; bh.length = 10</span>\n",
    "* What is the shape of the output layers weight vector Wo, and its bias vector bo?\n",
    "<span style=\"color:gold\">Wo = 50 x 3; bo.length = 3</span>\n",
    "* What is the shape of the networks output matrix Y?\n",
    "<span style=\"color:gold\">m x 3</span>\n",
    "* Write the equation that computes the networks output matrix Y as a function of X, Wh, bh, Wo and bo.\n",
    "<span style=\"color:gold\">Y* = ReLU(ReLU(XWh + bh)Wo + bo)</span>\n",
    "\n",
    " \n",
    "7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "\n",
    "<span style=\"color:gold\">I need 1 neuron. I should use the logistic activation function. </span>\n",
    "\n",
    "<span style=\"color:gold\">If instead I want to tackle MNIST I should use 10 neurons and the softmax activation function.</span>\n",
    "    \n",
    "<span style=\"color:gold\">Housing problem: 1 neuron and ReLU or softplus, since I'd expect a positive output.</span>\n",
    "    \n",
    "8. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "<span style=\"color:gold\">It's an algorithm to compute the loss at every iteration. It handles one mini-batch at a time. After computing the prediction (forward pass) and the output error, using the derivatives (especially the chain-rule) the algorithm goes backward and weights how much every step has counted in the final loss value. In the end, it performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "    <span style=\"color:gold\">Difference backpropagation-reverse autodiff: backpropagation refers to the whole process of trainig an ANN using multiple backpropagation steps. Reverse-mode autodiff is just a technique to ompute gradients efficiently, and it happens to be used by backpropagation</span>\n",
    "\n",
    "9. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "    \n",
    "    <span style=\"color:gold\">Learning rate, mini-batch size, number of hidden layers, number of neurons per layer, optimizer, number of iterations and the activation function.\n",
    "    I can lower down the number of neurons per layer and hidden layers, regularize the learning rate gradually, decrease the mini-batch size and use the early stopping technique to stop the training when it has reached the optimal amount of iterations.</span>\n",
    "\n",
    "10. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, use early stopping, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_full_train, y_full_train), (X_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_full_train.shape = (60000, 28, 28)\n",
    "# x_test.shape = (10000, 28, 28)\n",
    "X_valid, X_train = X_full_train[:5000] / 255., X_full_train[5000:] / 255.\n",
    "y_valid, y_train = y_full_train[:5000], y_full_train[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "K.clear_session()\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 8s 4ms/step - loss: 438485786096392863744.0000 - accuracy: 0.4922 - val_loss: 2.3911 - val_accuracy: 0.1126\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates)) # Plot horizontal lines at each y from xmin to xmax.\n",
    "plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss starts shooting back up violently when the learning rate goes over 6e-1, so let's try using half of that, at 3e-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=3e-1),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_mnist_logs\\\\run_003'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "run_index = 3 # increment this at every run\n",
    "run_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.3079 - accuracy: 0.1096 - val_loss: 2.3030 - val_accuracy: 0.1126\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3027 - accuracy: 0.1129 - val_loss: 2.3014 - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3032 - accuracy: 0.1102 - val_loss: 2.3037 - val_accuracy: 0.1126\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 2.3033 - accuracy: 0.1090 - val_loss: 2.3015 - val_accuracy: 0.1126\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3035 - accuracy: 0.1079 - val_loss: 2.3048 - val_accuracy: 0.1126\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 2.3037 - accuracy: 0.1078 - val_loss: 2.3042 - val_accuracy: 0.1126\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3034 - accuracy: 0.1068 - val_loss: 2.3015 - val_accuracy: 0.1126\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 2.3034 - accuracy: 0.1077 - val_loss: 2.3040 - val_accuracy: 0.0986\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3035 - accuracy: 0.1090 - val_loss: 2.3034 - val_accuracy: 0.0976\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.3035 - accuracy: 0.1084 - val_loss: 2.3032 - val_accuracy: 0.0986\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 2.3037 - accuracy: 0.1078 - val_loss: 2.3026 - val_accuracy: 0.1100\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3035 - accuracy: 0.1083 - val_loss: 2.3029 - val_accuracy: 0.1126\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 2.3039 - accuracy: 0.1084 - val_loss: 2.3018 - val_accuracy: 0.1126\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3033 - accuracy: 0.1089 - val_loss: 2.3038 - val_accuracy: 0.1126\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 2.3039 - accuracy: 0.1086 - val_loss: 2.3019 - val_accuracy: 0.1126\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3034 - accuracy: 0.1090 - val_loss: 2.3026 - val_accuracy: 0.0986\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 2.3036 - accuracy: 0.1066 - val_loss: 2.3016 - val_accuracy: 0.1126\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 2.3029 - accuracy: 0.1074 - val_loss: 2.3028 - val_accuracy: 0.1126\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.3032 - accuracy: 0.1104 - val_loss: 2.3065 - val_accuracy: 0.0868\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 2.3033 - accuracy: 0.1076 - val_loss: 2.3043 - val_accuracy: 0.1100\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3033 - accuracy: 0.1065 - val_loss: 2.3024 - val_accuracy: 0.1100\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 2.3034 - accuracy: 0.1080 - val_loss: 2.3030 - val_accuracy: 0.1100\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3025 - accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.30248761177063, 0.09799999743700027]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_mnist_model.h5\") # rollback to best model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
