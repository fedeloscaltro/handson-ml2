{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "central-virginia",
   "metadata": {},
   "source": [
    "# Problems:\n",
    "\n",
    "- vanishing gradients (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train.\n",
    "- Second, you might not have enough training data for such a large network, or it might be too costly to label.\n",
    "- Third, training may be extremely slow.\n",
    "- Fourth, a model with millions of parameters would severely risk overfitting the training set\n",
    "\n",
    "## Vanishing/Exploding Gradients Problems\n",
    "- <u>Vanishing</u>: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution.\n",
    "- <u>Exploding</u>: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges\n",
    "\n",
    "### Glorot and He Initialization\n",
    "We need the variance of the outputs of each layer to be equal to the variance of its inputs,2 and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
    "\n",
    "Not possible to guarantee both unless the layer has an equal number of inputs and neurons (_fan-in_ and _fan-out_ of the layer)\n",
    "\n",
    "So the connection weights of each layer must be initialized randomly as described in the next equation, where fan_avg = fan_in + fan_out /2.\n",
    "\n",
    "<img src=\"Xavier.jpg\" alt=\"Xiavier/Glorot initialization\" />\n",
    "\n",
    "Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the current success of Deep Learning.\n",
    "\n",
    "<img src=\"Table_initialization.jpg\" alt=\"Table of initializations\" />\n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution. You can change this to He initialization by setting ```python kernel_initializer=\"he_uniform\"``` or ```python kernel_initializer=\"he_normal\"```:\n",
    "\n",
    "```python\n",
    "    keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "```\n",
    "\n",
    "If you want He initialization with a uniform distribution, but based on fanavg rather than fanin, you can use the VarianceScaling initializer like this:\n",
    "\n",
    "```python\n",
    "    he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', \n",
    "                                                     distribution='uniform')\n",
    "    keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```\n",
    "\n",
    "## Nonsaturating Activation Functions\n",
    "dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0, especially if you used a large learning rate.\n",
    "\n",
    "A variant of the ReLU: the leaky ReLU: defined as LeakyReLUα(z) = max(αz, z).\n",
    "\n",
    "The hyperparameter α defines how much the function “leaks”: it is the slope of the function for z < 0, and is typically set to 0.01. setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01.\n",
    "\n",
    "Exponential linear unit (ELU) outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set.\n",
    "<img src=\"ELU-1.jpg\" alt=\"Exponential Linear Unit\" />\n",
    "<img src=\"ELU-plot.jpg\" alt=\"ELU plot\" />\n",
    "\n",
    "Major differences with ReLU:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-viewer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
