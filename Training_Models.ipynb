{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "To train that kind of model we need to find the value of Theta that minimizes the RMSE but in practise it's easier to to do so with the MSE.\n",
    "### Computational Complexity\n",
    "The SVD approac used by Scikit-Learn's LinearRegression class is abvout O(n^2), while the Normale Equation ranges between O(n^2.4) to O(n^3).\n",
    "\n",
    "Both get very slow when the number of features grows large (e.g. 100k) but both are linear with regard to te number of istances in the train set (O(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Capable of finding optimal solutions to a wide range of problems. It measures the local gradient of the error function wirh regard to the parametere vector Theta and it goeas in the direction of descending gradient.\n",
    "\n",
    "Learning rate --> hyperparameter that determinesthe size of the steps.\n",
    "\n",
    "MSE cost function: convex function so there are no local minima but just a global minimum.\n",
    "\n",
    "When using GD all the features shoud have a similar scale to speed up the convergence --> use StandardScaler\n",
    "\n",
    "### Batch GD\n",
    "The gradient vector contains all the partial derivatives (one for each model parameter).\n",
    "\n",
    "Solution: set a veru large number of iterations but interrupt the algorithm when the gradient vector becomes tiny, i.e. when its norm becomes smaller than a tiny number epsilon, or tolerance.\n",
    "## Stochastic GD\n",
    "It picks a random instance in the training set at every step and computes the gradients base only on that single instance.\n",
    "\n",
    "It will end up very close to the minimum but once there it continues to bounce arund, never settling down\n",
    "\n",
    "Solution: reduce gradually the learning rate\n",
    "\n",
    "Hint: training instances must be indipendent and identically distributed (IID) to ensure that the parameters get pulled toward the global optimum, on average --> shuffle the instances during training (e.g. pick each instance randomly)\n",
    "## Mini-batch GD\n",
    "It computes the gradients on small random sets of instances called mini-batches.\n",
    "\n",
    "It will end up walinkng around a bit closer to the minimum than Stochastic GD\n",
    "<center>\n",
    "    <img src=\"GD-comparison.JPG\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "Adds powers of each feature as new features, then train a linear model on this extended set of features.\n",
    "\n",
    "Use Scikit-Learn's PolynomialFeatures class to transform our training data, adding the square of each feature in the train set as a new feature.\n",
    "\n",
    "It's capable of finding relationships between features. It's possible by the fact that PolynomialFeatures also adds all combinations of fratures up to the given degree\n",
    "\n",
    "## Learning Curves\n",
    "\n",
    "If a model performs well on the trainif data but generalizzes poorlu accordinf to the cross-validation metrics, then your model is overfitting. If the performances are poor on bot, it's underfitting.\n",
    "\n",
    "Aternative: learning curves.\n",
    "\n",
    "Underfitting: use a more complex model or come up with better features\n",
    "\n",
    "Gap between curves: hall-mark of an overfitting model --> feed the model wit more training data until the validation error reaces the training error\n",
    "\n",
    "### Bias/Variance Trade-Off\n",
    "- Bias: due to wrong assumptions. If high ==> underfitting the training data\n",
    "- Variance: due to model's excessive sensitivity to small variations in the training data. If high ==> overfitting training data\n",
    "- Irreducible error: due to the noisiness of the data itself. Solution: clean up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models\n",
    "Polynomial model: One way: reduce number of polynomial degrees\n",
    "\n",
    "Linear model: constrain the weights of the model.\n",
    "\n",
    "Three ways.\n",
    "### Ridge Regression\n",
    "A regularization term is added to the cost function, only during training.\n",
    "\n",
    "It's common for the cost fdunction used during training to be different from the permformance measure used for testing.\n",
    "\n",
    "ALERT: important to scale the data before performing Ridge Regression. \n",
    "\n",
    "Performable either by computing a closed-form equation or by performing GD.\n",
    "\n",
    "The _penalty_ hyperparameter sets the type of regularization term ro use\n",
    "### Lasso Regression\n",
    "Instead of L2 norm used in Ridge, it has the L1 norm of the weight vector.\n",
    "\n",
    "It tends to eliminate the weights of the least important features ==> automatically performs feature selection and outputs a sparse model.\n",
    "\n",
    "ALERT: to avoid GD from bouncing around the optimum when using Lasso, gradually reduce the learining rate during training\n",
    "### Elastic Net\n",
    "Middle ground between Ridge and Lasso.\n",
    "\n",
    "Regularization term: mix between both.\n",
    "Elastic > Lasso because Lasso may behave erratically when the number of features is grater than the number of trainig instances or when several features are strongly correlated\n",
    "\n",
    "Generally: \n",
    "- <b> avoid plain Linear regression </b>\n",
    "- Good default: Ridge but if you know some features are useless then go with Lasso or elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Idea: stop training as soon as the validation error reaches a minimum.\n",
    "Validation error starts to bump up: the model has tstarted to overfit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Commonly used to estimate the probability that an instance belong to a particular class.\n",
    "#### Estrimating Probabilities\n",
    "It computes a weighted sum of the input features and it poutputs the _logistic_ of the result\n",
    "#### Training and Cost Function\n",
    "The cost function is convex, so GD is guaranteed to dind the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundaries\n",
    "When te model estimates a an equal probability between two or more classes.\n",
    "\n",
    "Logistic Regression models can be regularized using L1 or L2 penalties. Scikit-Learn adds an L2 by default.\n",
    "\n",
    "Parameter C of LogisticRegression: controls the regularization strength. The higher the value of C, the less the model is regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "First computes a score for each class k, then estimates the probability of each class by applying the softmax function.\n",
    "\n",
    "Each class has its own dedicated parameter vector Thta(k) typically stored as rows in a parameter matrix.\n",
    "\n",
    "The _argmax_ operator returns the value od a variable that maximizes a function\n",
    "\n",
    "ALERT: use softmax regression just with mutually exclusive classes.\n",
    "\n",
    "Minimizing the cost function --> Cross-Entropy function: pensalizes the model when it estimates a low probability for a target class. If an assumption is wrong, cross entropy will be grater by an amount called the Kullback.Leibler divergence\n",
    "\n",
    "\n",
    "Settable using LogisticRegression with the _multiclass_ hyperparameter to \"multinomial\" and specifying a solver that supports Softmax Regression such as the \"lbfgs\" solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "1. Which Linear regression training algorithm can you use if you have a training set with millions of features? \n",
    "\n",
    "<span style=\"color:gold\">Gradient Descent</span>\n",
    "\n",
    "2. Suppose the features in your training set have very different scales. What algorithms\n",
    "might suffer from this, and how? What can you do about it?\n",
    "\n",
    "<span style=\"color:gold\">Linear regression with Ridge Regression (sensitive to the scale of the input features) and Gradient Descent (it will take much longer to converge). I can use a StandardScaler object to solve the problem. It normalizes the data</span>\n",
    "\n",
    "3. Can Gradient Descent get stuck in a local minimum when training a Logistic\n",
    "Regression model?\n",
    "\n",
    "<span style=\"color:gold\">No because the loss function of the GD is convex, which means no local minima but just a global minimum</span>\n",
    "\n",
    "4. Do all Gradient Descent algorithms lead to the same model provided you let\n",
    "them run long enough?\n",
    "\n",
    "<span style=\"color:gold\">Kind of. Butunless the learning rate is gradually reduced, Stochastic and Mini-Batch will never converge</span>\n",
    "\n",
    "5. Suppose you use Batch Gradient Descent and you plot the validation error at\n",
    "every epoch. If you notice that the validation error consistently goes up, what is\n",
    "likely going on? How can you fix this?\n",
    "\n",
    "<span style=\"color:gold\">The model is overfitting. I can change the learning rate or definitely stop training</span>\n",
    "\n",
    "6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation\n",
    "error goes up?\n",
    "\n",
    "<span style=\"color:gold\">Nope because it's normal. The model has to learn how to predict the new data so I'd let it go through some time</span>\n",
    "\n",
    "7. Which Gradient Descent algorithm (among those we discussed) will reach the\n",
    "vicinity of the optimal solution the fastest? Which will actually converge? How\n",
    "can you make the others converge as well?\n",
    "\n",
    "<span style=\"color:gold\">Stochastic Gradient Descent but no, it will not converge. It will keep bumping around. Batch GD has no problem since it's the slowest but its error function is convex. The Mini-batch GD could be regularized</span>\n",
    "\n",
    "8. Suppose you are using Polynomial Regression. You plot the learning curves and\n",
    "you notice that there is a large gap between the training error and the validation\n",
    "error. What is happening? What are three ways to solve this?\n",
    "\n",
    "<span style=\"color:gold\">The model is overfitting the training data. I can feed the model with more training data until the validation error reaches the training error or reduce the number of polynomial degrees or I can apply early stopping. I can even add a L2 or L1 penalty to the cost function. Lastly I can increase the training set' size</span>\n",
    "\n",
    "9. Suppose you are using Ridge Regression and you notice that the training error\n",
    "and the validation error are almost equal and fairly high. Would you say that the\n",
    "model suffers from high bias or high variance? Should you increase the regularization\n",
    "hyperparameter α or reduce it?\n",
    "\n",
    "<span style=\"color:gold\">High bias. I should reduce α</span>\n",
    "\n",
    "10. Why would you want to use:\n",
    "• Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
    "\n",
    "<span style=\"color:gold\">Because it at least gives a bit of regularization</span>\n",
    "\n",
    "• Lasso instead of Ridge Regression?\n",
    "\n",
    "<span style=\"color:gold\">If some features are useless the Lasso could be really helful since it reduces the weights of the least important features</span>\n",
    "\n",
    "• Elastic Net instead of Lasso?\n",
    "\n",
    "<span style=\"color:gold\">Because Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated</span>\n",
    "\n",
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
    "Should you implement two Logistic Regression classifiers or one Softmax Regression\n",
    "classifier?\n",
    "\n",
    "<span style=\"color:gold\">Two Logistic Regression classifiers. The classes are not mutually indipendent</span>\n",
    "\n",
    "12. Implement Batch Gradient Descent with early stopping for Softmax Regression\n",
    "(without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
